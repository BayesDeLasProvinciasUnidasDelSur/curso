\documentclass[10pt]{article}
\input{../auxiliar/tex/encabezado.tex}
\input{../auxiliar/tex/tikzlibrarybayesnet.code.tex}

\newif\ifen
\newif\ifes
\newcommand{\en}[1]{\ifen#1\fi}
\newcommand{\es}[1]{\ifes#1\fi}
\entrue

\title{\huge Verdades empíricas: \\ inferencia probabilística causal. \\[0.4cm]  \LARGE Programa}

\author{Docente a cargo: Gustavo Landfried$^{1,2,3}$}
\affil{\small 1. Departamento de Computación. \\ Facultad de Ciencias Exactas y Naturales.  \\ Universidad de Buenos Aires. }
\affil{\vspace{-0.2cm}\small 2. Laboratorios de Métodos Bayesianos}
\affil{\vspace{-0.2cm}\small 3. Bayes Plurinacional}
\affil[]{Correspondencia: \texttt{glandfried@dc.uba.ar}}

\begin{document}

\maketitle

\begin{abstract}
Las verdades en ciencias con datos deben cumplir con el principio de ``no mentir'': no afirmar más de lo que se sabe (maximizando incertidumbre) sin ocultar lo que sí se sabe (dada la información disponible).
A pesar de que las reglas de la probabilidad formalizadas desde el siglo 18 garantizan la preservación de este principio, y que en todo este tiempo no se ha propuesto nada mejor en términos prácticos, el costo computacional asociados a la evaluación de todo el espacio de hipótesis ha limitado en la historia la aplicación estricta de este razonamiento para contextos de incertidumbre (aka enfoque bayesiano).

% Parrafo

Si bien durante el siglo 19 la física estadística logra resolver de forma completa una gran cantidad de problemas, durante el siglo 20 se deja de lado la pretensión de evaluar todo el espacio de hipótesis y se comienzan a desarrollar métodos que seleccionan una única hipótesis mediante alguna función de costo ad-hoc (e.g~como máxima verosimilitud).
Sin embargo, esto tuvo como consecuencia la emergencia de inestabilidades conocidas como \emph{overfitting}, que no existen bajo la aplicación estricta de las reglas de la probabilidad.

% Parrafo

En las vísperas del siglo 21, el crecimiento en la capacidad de cómputo levantó finalmente el principal obstáculo que pesaba sobre la aplicabilidad de las reglas de la probabilidad.
Con ellas se produce naturalmente una tendencia a favor de la agregación epistémica: las hipótesis individuales se agregan para formar variables, las variables se relacionan para formar modelos causales y los modelos causales se coordinan para formar teorías causales.
Justamente porque las teorías causales son las agregaciones de modelos causales que, al adaptarse a los diversos contextos, tienen mejor capacidad predictiva que modelos causales individuales.

% Parrafo

El objetivo del curso ``Verdades empíricas'' es revisar los métodos que se fueron desarrollando en las últimas décadas, que permiten aprovechar toda la potencia del aprendizaje automático probabilístico de forma flexible e intuitiva, para crear modelos personalizados a la medida de cada problema específico.
Con ellos podemos computar la incertidumbre óptima dada la información disponible, expresando las relaciones causales entre las variables de forma gráfica, descomponiendo las reglas de la probabilidad como mensajes entre los nodos de la red causal, y delegando la inferencia a los lenguajes de programación probabilística.


\end{abstract}

\section{Objetivos}

La ciencia tiene pretensión de verdad: alcanzar acuerdos intersubjetivos con validez universal.
Las ciencias formales (matemática, lógica) alcanzan estos acuerdos derivando teoremas dentro de sistemas axiomáticos cerrados.
Por el contrario, las ciencias empíricas (desde la física hasta las ciencias sociales) deben validar sus proposiciones en sistemas abiertos que contienen siempre algún grado de incertidumbre.
¿Es posible entonces alcanzar acuerdos intersubjetivos (``verdades'') en las ciencias empíricas en las que es inevitable decir ``no sé''?
Sí.
Podemos evitar mentir: no afirmar más de lo que se sabe (maximizando incertidumbre) sin ocultar lo que sí se sabe (dada la información disponible).

% Parrafo

Las reglas de la probabilidad se formalizaron a finales del siglo 18 y desde entonces se las ha adoptado como sistema de razonamiento en todas las ciencias empíricas.
Ellas son conceptualmente intuitivas: preservar la creencia previa que sigue siendo compatible con los datos (regla del producto) y predecir con la contribución de todas las hipótesis (regla de la suma).
A lo largo de todos estos siglos, las reglas de la probabilidad han sido derivadas repetidas veces a partir de diversos sistemas axiomáticos~\cite{halpern2017}.
A pesar de que ellas garantizan la preservación de los acuerdos intersubjetivos en contextos de incertidumbre (verdades en ciencias empíricas), y que en todo este tiempo no se ha propuesto nada mejor en términos prácticos, el costo computacional asociados a la evaluación de todo el espacio de hipótesis ha limitado históricamente la aplicación estricta de las reglas de la probabilidad (aka enfoque bayesiano).

% Parrafo

El primer ciclo de soluciones llega de la mano de física estadística, en la segunda mitad del siglo 19.
En esta época se desarrollan métodos analíticos para estimar la probabilidad de los micro-estados dada alguna información macro, como la energía, la temperatura, o algún otro promedio observable.
Conceptualmente se ha mostrado que las soluciones que ofrece la física estadística son equivalentes a las que se obtiene con el enfoque bayesiano a través del principio de ``no mentir'' (maximizar entropía dada la información disponible)~\cite{jaynes1957-informationTheoryAndStatisticalMechanics, jaynes2003-bookProbabilityTheory}.

% Parrafo

Durante el siglo 20 se deja de lado la pretensión de evaluar todo el espacio de hipótesis y se comienzan a desarrollar métodos que seleccionan una única hipótesis mediante alguna función de costo ad-hoc (e.g~como máxima verosimilitud).
Estas aproximaciones puntuales, al evitar evaluar todo el espacio de hipótesis, permitieron extender en la práctica el campo de aplicaciones estadísticas~\cite{friedman2001-elementsOfStatisticalLearning}.
Sin embargo, seleccionar una única hipótesis en contextos de incertidumbre tiene como consecuencia desafortunada la emergencia de inestabilidades conocidas como \emph{overfitting}, presentes en muchos modelos de aprendizaje automáticos actuales, pero que no están presentes en el sistema de razonamiento basado en la aplicación estricta de las reglas de la probabilidad~\cite{bishop2006-PRML}.

% Parrafo

En las vísperas del siglo 21, el crecimiento en la capacidad de cómputo levantó finalmente el principal obstáculo que pesaba sobre la aplicabilidad de las reglas de la probabilidad.
En esta época se desarrollan los métodos de aproximación por muestreo (basados en Cadenas de Markov) y los métodos analíticos de minimización de la divergencias entre distribuciones de probabilidad (expectation propogation, variational inference)~\cite{bishop2006-PRML}.
Por primera vez desde el descubrimiento de la teoría de la probabilidad, comienza a ser posible en todos los campos de las ciencia resolver la inferencia de los modelos probabilísticos aplicando estrictamente las reglas de la probabilidad, evaluando todo el espacio de hipótesis (aka~enfoque bayesiano).

% Parrafo

¿Pero qué es la inteligencia?
La inteligencia no puede reducirse a la verdad, pues no todas las verdades son igualmente relevantes.
Los problemas de conocimiento científico están, directamente o indirectamente, vinculados a un conjunto de problemas reales, y de ellos obtiene su relevancia y jerarquía.
El concepto mismo de ``problema'' sólo adquiere sentido en relación a algún sistema vivo.
Los problemas de la física y la química existen en tanto se subsumen como relevantes para los procesos de la vida.
En este sentido, podemos describir a la inteligencia como la capacidad que tienen las formas de vida para sobrevivir a través de los diversos contexto históricos y evolutivos.

% Parrafo

El problema de la comunicación con la realidad es el problema fundamental de la vida y por lo tanto de toda ciencia empírica.
Del mismo modo en que la evaluación de las hipótesis en probabilidad se hace a través de la secuencias de predicciones (siendo la sorpresa la única fuente de información~\cite{shannon1948-theoryOfCommunication}), la selección de las formas de vida en evolución también sigue un proceso de naturaleza multiplicativa, por secuencia de tasas de supervivencia y reproducción~\cite{taylor1978-replicatorDynamic,harper2009-replicatorAsInference,shalizi2009-replicatorAsInference, czegel2019-bayesianEvolution, czegel2022-bayesDarwin}.
Debido a que en los procesos multiplicativos los impactos de las pérdidas son más fuertes que los de las ganancias (por ejemplo, un único cero en la secuencia produce una extinción irreversible) existe una ventaja a favor de las variantes que reducen las fluctuaciones: por diversificación individual (\emph{propiedad epistémica}), por cooperación (\emph{propiedad evolutiva}), por especialización cooperativa (\emph{propiedad de especiación}), y por heterogeneidad cooperativa (\emph{propiedad ecológica}).

% Parrafo

Debido a que la teoría de toma de decisiones de Von Neumann está mal fundada, pues se base en funciones de costo ad-hoc (la utilidad esperada), todavía se sigue postulando en la teoría de juegos que la evolución de la cooperación está condicionada por un dilema (el dilema del prisionero).
%
Sin embargo, ya en el artículo ``A new interpretation of information rate'' John L Kelly~\cite{kelly1956} (colega de Shannon) se ve obligado a apartarse de la teoría de Von Neumann para resolver cuál es la apuesta óptima en proceso multiplicativos.
%
\begin{quotation}
The cost function approach [..] can actually be used to analyze nearly any branch of human endeavor.
The utility theory of Von Neumann shows us one way to obtain such a cost function.
The point here is that an arbitrary combination of a statistical transducer (i.e., a channel) and a cost function does not necessarily constitute a communication system.
What can be done, however, is to take some real-life situation which seems to possess the essential features of a communication problem, and to analyze it without the introduction of an arbitrary cost function.
\end{quotation}
%
La teoría de decisiones ha sido reformula recientemente, incorporando el concepto de ergodicidad~\cite{peters2019-ergodicityEconomics, peters2021-interpretation}, lo que permite evaluar lo que nos va a ocurrir en el tiempo, que depende de la naturaleza del problema que estamos analizando.

% Parrafo

La evolución de la cooperación no está condicionada por un dilema justamente debido a que en los procesos de selección multiplicativos, los individuos desertores aumentan las fluctuaciones de los cooperadores de quienes dependen, aumentando por lo tanto sus propias fluctuaciones y afectando negativamente su propia tasa de crecimiento de largo plazo (\emph{propiedad mayor})~\cite{GustavoLandfried}.
Si bien ciertas coyunturas pueden favorecer a la deserción en el corto plazo (\emph{propiedad menor}), la emergencia de unidades cooperativas de nivel superior es un fenómeno permanente en la historia de la vida (transiciones evolutivas mayores) \cite{maynardSmith}.
La evidencia es nuestra propia vida, que depende de al menos cuatro de estos niveles para sobrevivir: la célula con la mitocondria, el organismo multicelular, el sistema social y la comunidad ecológica.

% Parrafo

Esta tendencia hacia la agregación de unidades no ocurre solo con la información genética, también ocurre con la información cultural o científica.
Así es que las hipótesis individuales se agregan para formar variables, las variables se relacionan para formar modelos causales y los modelos causales se coordinan para formar teorías causales~\cite{pearl2009-causality}.
Y esta tendencia a la agregación epistémica emerge naturalmente de la aplicación estricta de las reglas de la probabilidad~\cite{winn2012-causality}, en particular del proceso multiplicativo de evaluación de hipótesis por secuencias de predicciones.
Justamente las teorías causales son las agregaciones de modelos causales que, al adaptarse a los diversos contextos, tienen mejor capacidad predictiva que modelos causales individuales~\cite{winn2012-causality}.

% Parrafo

El objetivo del curso ``Verdades empíricas'' es revisar los métodos actuales que permiten implementar modelos a medida del problema de forma sencilla e intuitiva.
En las últimas décadas se fueron desarrollando técnicas que permiten aprovechar toda la potencia del aprendizaje automático probabilístico~\cite{murphy-pmlBook1,murphy-pmlBook2} de forma flexible e intuitiva, para crear modelos personalizados a la medida de cada problema específico.
Con ellos podemos computar la incertidumbre óptima dada la información disponible, expresando las relaciones causales entre las variables de forma gráfica, descomponiendo las reglas de la probabilidad como mensajes entre los nodos de la red causal, y delegando la inferencia a los lenguajes de programación probabilística.


\section{Contenidos mínimos}

Acuerdos intersubjetivos en contextos de incertidumbre. Las reglas de la probabilidad. Especificación gráfica de modelos causales. Evaluación de modelos causales.

Sorpresa: el problema de la comunicación con la realidad. La estructura invariante del dato empírico. Tasa de predicción e información. Evaluación de sistemas de comunicación alternativos en base a su tasa de sorpresa. Entropía y entropía cruzada. Distribuciones de probabilidad exponencial.

Algoritmo eficientes de inferencia en modelos gráficos (sum-product algorithm sobre factor graphs). Introducción a las Aproximaciones analíticas (expectation propagation). Lenguajes de programación probabilísticos. Modelos de estimación de habilidad y evaluación de test diagnósticos.

Inferencia causal. Los niveles del razonamiento causal. Flujos de inferencia en modelos causales. Efecto de las intervenciones en modelos causales. Conclusiones causales a partir de observaciones. Identificación de modelo causal mediante intervenciones. Buenos y malos controles.

Series de tiempo. El problema de usar el posterior como prior del siguiente evento en el modelo de estimación de habilidad. Los modelos gráficos de historia completa. El algoritmos de pasaje de mensajes iterativos y su convergencia. Consideraciones de inferencia causal en series temprales.

Toma de decisiones. Naturaleza multiplicativa de los procesos de selección probabilística y evolutiva. Sus consecuencias. La ventaja a favor de las variantes que reducen las fluctuaciones: diversificación individual (propiedad epistémica), cooperación (propiedad evolutiva mayor), especialización (propiedad meta-epistémica), heterogeneidad (propiedad ecológica).

Competencia por grupos. Presentación de una competencia de inferecia con apuestas e intercambio de recursos.


\begin{enumerate}

\item \textbf{Modelos gráficos e inferencia}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] Acuerdos intersubjetivos en contextos de incertidumbre. Las reglas de la probabilidad. Especificación gráfica de modelos causales. Evaluación de modelos causales.
\item[Laboratorio:] Simulación de procesos generativos y evaluación de modelos causales alternativos (Monty Hall). Apuestas óptimas individuales en procesos multiplicativos (casa de apuesta por la moneda). Implementar numeros seudo-aleatorios. Galton Board. Polya Bord.
\end{description}


\item \textbf{Algoritmos eficientes de inferencia}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] El problema histórico de las reglas de la probabilidad. Distribuciones conjugadas gaussiana y beta-binomial. La emergencia del sobreajuste y el balance natural de las reglas de la probabilidad.
\item[Laboratorio:] Implementación de modelos de recomendación Beta-Binomial. Implementación de regresión lineal bayesiana.
\end{description}

\vspace{0.1cm}
\item \textbf{Sorpresa: el problema de la comunicación con la realidad}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] La estructura invariante del dato empírico. Construcción de sistemas de comunicación con la realidad. Tasa de predicción e información. Evaluación de sistemas de comunicación alternativos en base a su tasa de sorpresa. Entropía y entropía cruzada. Distribuciones de probabilidad exponencial.
\item[Práctica:] Selección de modelo lineal bayesiano.
% Monedas con sesgo, basados en infecciones no observables, evaluados con diversos test diagnósticos. (Para después de lenguajes progrmación probabilística)
\end{description}


\vspace{0.1cm}
\item \textbf{Modelos gráficos y algoritmo de inferencia}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] Algoritmo eficientes de inferencia en modelos gráficos (sum-product algorithm sobre factor graphs). Introducción a las Aproximaciones analíticas (expectation propagation). Lenguajes de programación probabilísticos.
\item[Práctica:] Implementación estimación de habilidad de habilidad, Elo y TrueSkill. Implementación de evaluación de test diagnósticos.
\end{description}


\vspace{0.1cm}
\item \textbf{Inferencia causal}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] Los niveles del razonamiento causal. Flujos de inferencia en modelos causales. Efecto de las intervenciones en modelos causales. Conclusiones causales a partir de observaciones. Identificación de modelo causal mediante intervenciones.
\item[Práctica:] Buenos y malos controles en regresión lineal bayesianas. Selección de teorías causales alternativas (la rama y el árbol).
\end{description}

% Parrafo





% Parrafo

\vspace{0.1cm}
\item \textbf{Propiedades de la función de costo epistémico-evolutiva}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] Naturaleza multiplicativa de los procesos de selección probabilística y evolutiva. Sus consecuencias. La ventaja a favor de las variantes que reducen las fluctuaciones: diversificación individual (propiedad epistémica), cooperación (propiedad evolutiva mayor), especialización (propiedad meta-epistémica), coexistencia (propiedad ecológica). Las distribución de biomasa en la tierra.
\item[Práctica:] Simulación de un proceso multiplicativo, aproximación numérica del promedio de los estados y del promedio temporal. Análisis matemáticas de las apuestas. Proceso de Bienaymé-Galton-Watson.
\end{description}

% Parrafo



% Parrafo

\vspace{0.1cm}
\item \textbf{Distribucioens de creencias}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] Bernoulli, Binomial, Gaussiana, Poly Urn, Beta, Poison, Multinomial, Guasiana multivariada, etc. Regresión lineal multivariada.
\item[Práctica:] Implentar la clase guasiana. Regresión lineal. Buenos y malos controles.
\end{description}

% Parrafo

\vspace{0.1cm}
\item \textbf{Evaluación de modelo}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] Modelos gaussianos multidimensionales. Regresión lineal bayesiana. El enfoque frecuentista y el sobreajuste \emph{overfitting}. Evidencia. Media
\item[Práctica:] Buenos y malos controles.
\end{description}

% Parrafo

\vspace{0.1cm}
\item \textbf{Aproximaciones analíticas}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] Modelos de estimación de habilidad de la industria del video juego. Modelo Elo. Modelo TrueSkill. Aproximación por \emph{expectation propagation}. Modelo bayesiano de recomendación de Netflix. Aproximación por \emph{variational inference}.
\item[Práctica:] Implementación del Modelo Elo y el modelo TrueSkill. Estimación de habilidad y comparación.
\end{description}

% Parrafo

\vspace{0.1cm}
\item \textbf{Datos secuenciales y series de tiempo}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] El problema de usar el posterior como prior del siguiente evento en el modelo de estimación de habilidad. Los modelos gráficos de historia completa. El algoritmos de pasaje de mensajes iterativos y su convergencia.
\item[Práctica:] Implementación de modelos secuenciales sencillos. Verificación del problema. Implementación de su solución.
\end{description}

% Parrafo

\vspace{0.1cm}
\item \textbf{Aproximaciones por exploración}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] Cadenas de Markov. Algoritmos de muestreo básicos. Métodos de aceptación y rechazo. Importance sampling. Cadenas de markov. El algoritmo Metrópolis-Hasting. Gibbs Sampling.
\item[Práctica:] Implementación del algoritmo Metrópolis-Hasting
\end{description}

% Parrafo

\vspace{0.1cm}
\item \textbf{Lenguajes de programación probabilística}
\vspace{-0.15cm}
\begin{description}
\item[Teórica:] Implementación de modelos usando PPLs. Verificaciones de buen funcionamiento. 
\item[Práctica:] Implementación de varios modelos usando PPLs.
\end{description}


\end{enumerate}

\nocite{jaynes1984-bayesianBackground, mcelreath2020-rethinking, bishop2006-PRML, pearl2009-causality, cinelli2021-crashCourse, stan-userGuide, martin2021-BMCP, samaja1999-epistemologiaMetodologia }

{\bibliographystyle{../auxiliar/biblio/plos2015}
\bibliography{../auxiliar/biblio/biblio_notUrl.bib}
}

\end{document}
