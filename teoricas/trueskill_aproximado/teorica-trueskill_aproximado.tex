\documentclass[shownotes,aspectratio=169]{beamer}

\input{../../aux/tex/diapo_encabezado.tex}
\input{../../aux/tex/tikzlibrarybayesnet.code.tex}
 \mode<presentation>
 {
 %   \usetheme{Madrid}      % or try Darmstadt, Madrid, Warsaw, ...
 %   \usecolortheme{default} % or try albatross, beaver, crane, ...
 %   \usefonttheme{serif}  % or try serif, structurebold, ...
  \usetheme{Antibes}
  \setbeamertemplate{navigation symbols}{}
 }
 
\usepackage{todonotes}
\setbeameroption{show notes}

\newif\ifen
\newif\ifes
\newcommand{\en}[1]{\ifen#1\fi}
\newcommand{\es}[1]{\ifes#1\fi}
\estrue

%\title[Bayes del Sur]{}

\begin{document}

\color{black!85}
\large
 
%\setbeamercolor{background canvas}{bg=gray!15}


\begin{frame}[plain,noframenumbering]
 
 \begin{textblock}{90}(00,05)
\begin{center}
 \huge  \textcolor{black!66}{Creencias adaptativas}
\end{center}

\begin{textblock}{47}(113,72)
\centering \Large  \textcolor{white!55}{TrueSkill} \ \ \ \ \ \
\end{textblock}
\begin{textblock}{47}(116,75)
\centering \Large \ \ \ \ \ \textcolor{white!55}{aproximado}
\end{textblock}

\end{textblock}

 %\vspace{2cm}brown
%\maketitle
\Wider[2cm]{
\includegraphics[width=1\textwidth]{../../aux/static/peligro_predador}
}
\end{frame}



\begin{frame}[plain]

\begin{textblock}{160}(0,6)
\begin{equation*}
\overbrace{P(s_1 \mid r, \text{Modelo})}^{\text{Posterior}} \propto \overbrace{\N(s_1 \, | \, \mu_1, \sigma_1^2) }^{\text{Prior}} \, \overbrace{1 - \Phi(\, 0\,| \, \delta - \mu_1 + s_1 , \vartheta^2 - \sigma_1^2)}^{\text{Verosimilitud}}  \ \  \text{Caso ganador} 
\end{equation*}
\end{textblock}

\begin{textblock}{160}(0,32)
\centering
\includegraphics[page=2,width=0.49\textwidth]{figures/posterior_win}
\end{textblock}

\end{frame}

\begin{frame}[plain]
\begin{textblock}{160}(0,4)
 \Large \centering Aproximaciones
\end{textblock}


\begin{columns}[t]
\begin{column}{0.5\textwidth}
 \centering \textbf{Inferencia variacional}
 
\begin{itemize}
  \item[$\circ$] Sesgado
  \item[$\circ$] R\'apido y escalable
 \end{itemize}

 
 \end{column}
 \begin{column}{0.5\textwidth}
\centering  \textbf{Cadenas de Markov}

\begin{itemize}
  \item[$\circ$] No sesgado
  \item[$\circ$] Lento y no escalable
 \end{itemize}

\end{column}
\end{columns}


\end{frame}


\begin{frame}[plain]
\begin{textblock}{160}(0,4)
\centering \Large Inferencia variacional
\end{textblock}

Encontrar la mejor aproximaci\'on a la posterior

\begin{equation*}
 p(\theta|x) \approx q(\theta)
\end{equation*}

\vspace{0.3cm}
Que minimice una distancia $D$
\begin{equation*}
 q(\theta) = \underset{q \in \mathcal{Q}}{\text{arg min}} \ \ D(q(\theta) \ || \ p(\theta|x))
\end{equation*}
\end{frame}
 
 

\begin{frame}[plain]
\begin{textblock}{160}(0,4)
\centering \Large Solcuiones
\end{textblock}

$\bullet$ \textbf{Variational Bayes}

\begin{equation*}
 q(\theta) = \underset{q \in \mathcal{Q}}{\text{arg min}} \ \ \text{KL}(q(\theta) \ || \ p(\theta|x))
\end{equation*}

\pause
\vspace{0.5cm}

$\bullet$ \textbf{Expecatation Propagation}

\begin{equation*}
 q(\theta) = \underset{q \in \mathcal{Q}}{\text{arg min}} \ \ \text{KL}(p(\theta|x) \ || \  q(\theta))
\end{equation*}

\end{frame}

\begin{frame}[plain]
 \begin{textblock}{160}(0,4)
\centering \Large Kullback-Leibler
\end{textblock}
\vspace{0.5cm}

\begin{columns}[t]
\begin{column}{0.5\textwidth}
\centering


 \begin{equation*}
 \text{KL}( p \ || \ q) = 
\int p(\theta|x) \log \frac{p(\theta|x)}{q(\theta)} d\theta
\end{equation*}

 \end{column}
 \begin{column}{0.5\textwidth}
\centering


\begin{equation*}
 \text{KL}(q \ || \ p) =  \int q(\theta) \log \frac{q(\theta)}{p(\theta|x)} d\theta
\end{equation*}

 
\end{column}
\end{columns}

 \vspace{0.5cm}

 \centering
 \includegraphics[width=0.7\textwidth]{../../aux/static/kl-divergence.png}
 
 
\end{frame}


\begin{frame}[plain]
 \begin{textblock}{160}(0,4)
\centering \Large Divergencia generalizada
\end{textblock}
\vspace{0.5cm}


\centering
\includegraphics[width=0.7\textwidth]{../../aux/static/d-divergence.png}

\vspace{0.3cm}

\includegraphics[width=1\textwidth]{../../aux/static/divergence.png}
 

\end{frame}


\begin{frame}[plain]
 \begin{textblock}{160}(0,4)
\centering \Large Propiedades
\end{textblock}
\vspace{0.5cm}

\centering
\includegraphics[width=1\textwidth]{../../aux/static/divergence2.png}


\end{frame}

\begin{frame}[plain]
 \begin{textblock}{160}(0,4)
\centering \Large Mean field approximation
\end{textblock}
\vspace{0.5cm}

Restricci\'on de independencia sobre $q$ para que se pueda factorizar,
\begin{equation*}
 q(\theta) = \prod_{i=1}^m q_i(\theta_i)
\end{equation*}

\textbf{Block coordinate assent}: En cada paso fijamos todos los factores $q_i(\theta_i)$ salvo uno, y optimizamos con respeto al restante $q_j(\theta_j)$
% 
% \begin{equation*}
%  \underset{q_j \in \mathcal{Q}}{\text{arg max}} \ \mathcal{L}(q_j(\theta_j)\prod_{i\neq j}^m q_i(\theta_i))
% \end{equation*}

\end{frame}


\begin{frame}[plain]
 \begin{textblock}{160}(0,4)
  \centering \Large Aproximaci\'on \\ \large incluyendo empates
 \end{textblock}

 \only<2>{
\begin{textblock}{160}(0,50)
\centering
En vez de aproximar el posterior \\ aproximamos la única distribución que no gaussiana
\end{textblock}
}

 
\only<2->{
\begin{textblock}{160}(0,20)
\begin{equation*}
p(d) =
\begin{cases}
\N(d|\delta,\vartheta^2) \mathbb{I}(-\varepsilon < d < \varepsilon) & \text{empate} \\
\N(d|\delta,\vartheta^2) \mathbb{I}(d > \varepsilon) & \text{no empate}
\end{cases}
\end{equation*}
\end{textblock}
}

\only<3->{
\begin{textblock}{160}(0,40)
\begin{align*}
 m_{d_1 \rightarrow f_{d_1}}(d_1)   = \frac{p(d_1)}{m_{f_{d_1} \rightarrow d_1}(d_1)} 
 & \onslide<4->{\approx \frac{\widehat{p}(d_1)}{m_{f_{d_1} \rightarrow d_1}(d_1)}}  \\
& \onslide<5-> {= \frac{\N(d_1 \,  | \,\widehat{\delta} , \, \widehat{\vartheta}^{\,2} )}{\N(d_1 | \delta, \vartheta^2)}}
\onslide<6->{\propto \N(d_1,\delta_{\div},\vartheta_{\div}^2 )}
\end{align*}
\onslide<7>{
\vspace{0.3cm}
\centering
\es{con} $\delta_{\div} = \frac{\widehat{\delta}}{\widehat{\vartheta}^2} - \frac{\delta}{\vartheta^2}$ \en{and}\es{y} $\vartheta_{\div}^2 = (\frac{1}{\widehat{\vartheta}^2} - \frac{1}{\vartheta^2})^{-1}$ }
\end{textblock}
}

 
\end{frame}

\begin{frame}[plain]
\begin{textblock}{160}(0,4)
 \centering \Large Momentos de la Gaussiana Truncada
\end{textblock}

 \begin{equation*}
 E(X| a < X < b) = \mu + \sigma \frac{\N(\alpha) - \N(\beta) }{\Phi(\beta) - \Phi(\alpha) }
\end{equation*}
%
\begin{equation*}
 V(X| a < X < b) = \sigma^2 \Bigg( 1 + \bigg(\frac{\alpha N(\alpha) - \beta N(\beta) }{\Phi(\beta) - \Phi(\alpha) }\bigg) - \bigg(\frac{N(\alpha) - N(\beta) }{\Phi(\beta) - \Phi(\alpha) }\bigg)^2 \Bigg)
\end{equation*}
%
\en{where $\beta = \frac{b-\mu}{\sigma}$ and $\alpha = \frac{a-\mu}{\sigma}$.}
\es{donde $\beta = \frac{b-\mu}{\sigma}$ y $\alpha = \frac{a-\mu}{\sigma}$.}

\end{frame}


\begin{frame}[plain]
\begin{textblock}{160}(0,4)
 \centering \Large Momentos de la Gaussiana Truncada
\end{textblock}

\begin{align*}
 E(X| a < X )  &=  \mu + \sigma \frac{\N(\alpha)}{1 - \Phi(\alpha) } \\ 
 V(X| a < X )  &= \sigma^2 \Bigg( 1 + \bigg(\frac{\alpha \N(\alpha)}{1 - \Phi(\alpha) }\bigg) - \bigg(\frac{\N(\alpha)}{1 - \Phi(\alpha) }\bigg)^2 \Bigg) 
\end{align*}
\end{frame}


\begin{frame}[plain]
\begin{textblock}{160}(0,4)
 \centering \Large Diferencia aproximada
\end{textblock}

\begin{equation*}
 \widehat{p}(d) = \N(d | \widehat{\delta}, \widehat{\vartheta}^2) =
 \begin{cases*}
 \N\Big(d \,  | \, E(d | -\varepsilon < d < \varepsilon ) , \,  V(d | -\varepsilon < d < \varepsilon ) \, \Big) & \text{tie} \\
\N\Big(d \,  | \, E(d | d > -\varepsilon ) , \,  V(d | d > -\varepsilon ) \, \Big) & \text{not tie}
  \end{cases*}
\end{equation*}
%

\end{frame}

\begin{frame}[plain]
\begin{textblock}{160}(0,4)
 \centering \Large $\widehat{m}_{f_{d_1} \rightarrow t_a}(t_a)$
 \end{textblock}

 
 
 \begin{equation*}
\begin{split}
\widehat{m}_{f_{d_1} \rightarrow t_a}(t_a) & =  \iint \mathbb{I}(d_1 = t_a - t_b) \N(d_1 | \delta_{\div}, \vartheta_{\div}^2) \N(t_b | \mu_b , \sigma_b^2 )  \, d{d_1} d_{t_b} \\
& = \int  \N( t_a-t_b | \delta_{\div}, \vartheta_{\div}^2) \N(t_b | \mu_b , \sigma_b^2 )  \,  d_{t_b} \\
& = \N(t_a \, | \, \mu_b + \delta_{\div} \, , \, \vartheta_{\div}^2 + \sigma_b^2) \\
\end{split}
\end{equation*}
\end{frame}

\begin{frame}[plain]
\begin{textblock}{160}(0,4)
 \centering \Large $\widehat{m}_{f_{t_a} \rightarrow p_1}(p_1)$
\end{textblock}
 \begin{equation*}
 \begin{split}
\widehat{m}_{f_{t_a} \rightarrow p_1}(p_1) &= \iint \mathbb{I}(t_a = p_1 + p_2) \N(t_a \, | \, \mu_b + \delta_{\div} \, , \, \vartheta_{\div}^2 + \sigma_b^2) \N(p_2 | \mu_2 , \sigma_2^2 + \beta^2)  \, d{t_a} d_{p_2} \\
& = \int \N(p_1 + p_2 \, | \, \mu_b + \delta_{\div} \, , \, \vartheta_{\div}^2 + \sigma_b^2) \N(p_2 | \mu_2 , \sigma_2^2+ \beta^2 )   \, d_{p_2} \\
& = \N( p_1 \,|\,  \underbrace{\mu_b - \mu_2}_{\mu_1-\delta} + \delta_{\div}  \,,\,\vartheta_{\div}^2 + \underbrace{\sigma_b^2 + \sigma_2^2 + \beta^2}_{\vartheta^2 - (\sigma_1^2 + \beta^2)})  \\
\end{split}
\end{equation*}
\end{frame}

\begin{frame}[plain]
\begin{textblock}{160}(0,4)
 \centering \Large Verosimilitud aproximada
\end{textblock}

 \begin{equation*}
\begin{split}
\widehat{m}_{f_{p_1} \rightarrow s_1}(s_1) & = \int \N(p_1|s_1,\beta^2) \N(p_1| \mu_1 - \delta + \delta_{\div}, \vartheta_{\div}^2 + \vartheta^2 - \sigma_1^2 - \beta^2)dp_1 \\
& = \N(s_1| \mu_1 - \delta + \delta_{\div}, \vartheta_{\div}^2 + \vartheta^2 - \sigma_1^2)
\end{split}
\end{equation*}
\end{frame}

\begin{frame}[plain]
\begin{textblock}{160}(0,4)
 \centering \Large Posterior aproximado
\end{textblock}
\begin{equation*}
 \widehat{p}(s_1, r) = \N(s_1|\mu_1, \sigma_1^2) \N(s_1| \mu_1 - \delta + \delta_{\div}, \vartheta_{\div}^2 + \vartheta^2 - \sigma_1^2)
\end{equation*}
\end{frame}


\begin{frame}[plain]
\centering
  \includegraphics[width=0.35\textwidth]{../../aux/static/pachacuteckoricancha.jpg}
\end{frame}


\end{document}



