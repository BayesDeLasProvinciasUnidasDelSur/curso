\documentclass[shownotes,aspectratio=169]{beamer}

\input{../../aux/tex/diapo_encabezado.tex}
\input{../../aux/tex/tikzlibrarybayesnet.code.tex}
 \mode<presentation>
 {
 %   \usetheme{Madrid}      % or try Darmstadt, Madrid, Warsaw, ...
 %   \usecolortheme{default} % or try albatross, beaver, crane, ...
 %   \usefonttheme{serif}  % or try serif, structurebold, ...
  \usetheme{Antibes}
  \setbeamertemplate{navigation symbols}{}
 }
 
\usepackage{todonotes}
\setbeameroption{show notes}

\newif\ifen
\newif\ifes
\newcommand{\en}[1]{\ifen#1\fi}
\newcommand{\es}[1]{\ifes#1\fi}
\estrue

%\title[Bayes del Sur]{}

\begin{document}

\color{black!85}
\large

 
%\setbeamercolor{background canvas}{bg=gray!15}

\begin{frame}[plain,noframenumbering]
 
 \begin{textblock}{90}(00,05)
\begin{center}
 \huge  \textcolor{black!66}{Creencias, datos y sorpresas}
\end{center}
\end{textblock}

 %\vspace{2cm}brown
%\maketitle
\Wider[2cm]{
\includegraphics[width=1\textwidth]{../../aux/static/peligro_predador}
}
\end{frame}
 
 % if we are told that a highly improbable event has just occurred, we will
% have received more information than if we were told that some very likely event
% has just occurred, and if we knew that the event was certain to happen we would
% receive no information. Our measure of information content will therefore depend
% on the probability distribution p(x), and we therefore look for a quantity h(x) that
% is a monotonic function of the probability p(x) and that expresses the information
% content. The form of h(·) can be found by noting that if we have two events x
% and y that are unrelated, then the information gain from observing both of them
% should be the sum of the information gained from each of them separately, so that
% h(x, y) = h(x) + h(y). Two unrelated events will be statistically independent and
% so p(x, y) = p(x)p(y).

 
\begin{frame}[plain]
\begin{textblock}{160}(0,4)
 \centering \LARGE 
 \en{Honesty optimizes information}
 \es{La honestidad óptimiza la información}
 \end{textblock}
 \vspace{1.4cm}

 \begin{equation*}
 \underbrace{\text{\en{Entropy}\es{Entropía}}(X)}_{\text{\en{Expected information}\es{Información esperada}}} = \ \sum_{x\in X} \ P(x) \  \cdot \underbrace{(-\log P(x))}_{\hfrac{\text{\scriptsize \en{Information generated}\es{Información generada}}}{\text{\scriptsize \en{by the surprise}\es{por la sorpresa}}}}
\end{equation*}

\pause

\vspace{0.3cm}

\begin{center}
\en{Maximum expected information $\Leftrightarrow$ Maximum uncertainty}
\es{M\'axima información esperada $\Leftrightarrow$ Máxima incertidumbre}
\end{center}

\vspace{0.7cm}

\pause

\Wider[-5cm]{
\begin{mdframed}[backgroundcolor=black!20]
\begin{equation*}
  \text{\en{Honesty}\es{Honestidad}} = \underset{P(X)}{\text{ arg max }} \text{\en{Entropy}\es{Entropía}}(X)
\end{equation*}
\end{mdframed}
}

\end{frame}


 
\begin{frame}[plain]
\centering
  \includegraphics[width=0.35\textwidth]{../../aux/static/pachacuteckoricancha.jpg}
\end{frame}






\end{document}



